---
title: "Markov chain Monte Carolo"
output:
  html_document: default
  html_notebook: default
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 6
---

#### From the github Bayeslabs:   
<https://github.com/nthobbs50/BayesLabs/blob/master/Labs/>   
and tutorial: <http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/>   

#### MCMC1.Rmd and MCMC1Math.pdf

```{r preliminaries, include=FALSE}
rm(list = ls())
library(knitr)
library(actuar)

knitr::opts_chunk$set(cache = FALSE, warnings = FALSE, tidy = TRUE)

# uncomment out this to generate key
 nokey = FALSE; key = TRUE

# uncomment out this to generate exercise
#nokey = TRUE; key = FALSE
```

#### Practice Problem

Markov chain Monte Carlo algorithm takes a multivariate joint distribution and breaks it into a series of univariate, marginal distributions that can be approximated one at a time.  

  1. I have measured the (_change in _) territory of _n_ = 15 bears roaming the front range foot hills and assumed their measured territory size y~i~ was normally distributed with a variance $\sigma$^2^~p~ = 10 km^2^. The average observed territory size in this study was $\mu$~y~ = 78.1 km^2^. Our study has a known [$\sigma$^2^~p~ | 78.1, 10]  
  2. This study has been conducted before, and the researchers determined that the average territory was $\mu$~0~ = 70 km^2^ with variance $\sigma$^2^~0~ = 12km^2^.(The prior distribution from the previous study's observation)   
  3. We wish to update our knowledge of $\mu$, the unobservable average bears' territory with our new data in a Bayesian framework.

#### MCMC Metropolis steps   
  1. Select a number, a guess, $\theta^{(k)}$.
  2. (based on/or forming?? the proposal distribution)

#### Markov Chain Monte Carolo Using Gibbs Sampling   
Unlike Metropolis/Metropolis-Hastings, Gibbs retains all and updates instaed of an accpet-reject method. 


#### II. Code

You will write code using conjugate relationships, also known as Gibbs updates, to draw samples from marginal posterior distributions of a mean and variance. 

     1. Write expression for posterior proportional to joint distribution
     2. Use previous code for joint distribution to write full-conditional distriubtion for each unobserved quantities
     3. WRite sampling procedures to make draws from full-conditional distribution of each of the unknowns. 
     4. Iterate over all distributions consecutively to sample a single parameter at each iteratiion (treat all other parameters as known and constant at that iteration.)
#### R specific 
1. Set the seed for random numbers = 1 in R with `set.seed(1)`. 

2. Load the `actuar` library, which contains functions for inverse gamma distributions needed in step 4. 

3. Simulate 100 data points from a normal distribution with mean $\theta = 100$ and variance $\varsigma^{2} = 25$.  You will use these data to test the conjugate functions that you will write in 4 below. Call the data set `y`. Be careful here. R requires the standard deviation, not the variance, as a parameter. Simulating data is always a good way to test methods. Your method should be able to recover the generating parameters given a sufficiently large number of simulated observations. 
  
```{r, tidy = TRUE, tidy.opts = list(width.cutoff = 100), echo = key, include = key}
set.seed(1)
library(actuar)
varsigma.sq = 25
theta = 100
n = 100

y = rnorm(n, theta, sqrt(varsigma.sq))

plot(density(y))

```

From the N.T. Hobbs lab: 
   "We have provided a function called `draw_mean` that 
calculates the parameters of the posterior distributions for $\theta$ using a normal-normal conjugate relationship where the variance is assumed to be known. The function returns a draw from that distribution. Study the notes relative to this function."

   "We provided a function called `draw_var` that
calculates the parameters of the posterior distributions for $\varsigma^2$ using a inverse gamma-normal conjugate relationship where the mean is assumed to be known. The function returns a draw from that distribution."

```{r, echo = TRUE, include = TRUE}

draw_mean = function(mu_0, sigma.sq_0, varsigma.sq, y){
	mu_1 =((mu_0 / sigma.sq_0 + sum(y)/varsigma.sq)) / 
	  (1/sigma.sq_0 + length(y)/varsigma.sq)
	sigma.sq_1 = 1/(1 / sigma.sq_0 + length(y) / varsigma.sq)
	z = rnorm(1, mu_1, sqrt(sigma.sq_1))
	param = list(z = z, mu_1 = mu_1, sigma.sq_1 = sigma.sq_1)
	return(param)
}

# normal likelihood with gamma prior conjugate relationship for variance, assuming mean is known
# alpha_0 is parameter of prior for variance
# beta_0 is parameter of prior for variance
# Note that this uses scale parameterization for inverse gamma

draw_var = function(alpha_0, beta_0, theta, y){
	alpha_1 = alpha_0 + length(y) / 2
	beta_1 = beta_0 + sum((y - theta)^2) / 2
	z = rinvgamma(1, alpha_1, scale = beta_1)
	param = list(z = z, alpha_1 = alpha_1, beta_1 = beta_1)
	return(param)
}

```



Check the functions by simulating a large number of data points from a normal distribution using a mean and variance of your own choosing.  Store the data points in a vector called `y_check`.  
Can this joint distribution retrieve your mean and sigma? Simulate!
```{r, echo=key, include=key}
check_mean=32; check_sigma = 3.2
check_var=check_sigma^2
n.draw=10000
y_check=rnorm(n.draw,mean=check_mean,sd=check_sigma)

var_vec = numeric(n.draw) # to make a numeric vector of length  equal to that of n.draw
mean_vec = numeric(n.draw)

for(i in 1:n.draw){
  var_vec[i] = draw_var(alpha_0=.001, beta_0=.001,theta=check_mean,y=y_check)$z
  mean_vec[i] = draw_mean(0,10000,check_sigma^2,y_check)$z
} 

#Take the mean of the parameters (Kery 2010, Gelman & Hill 2009) 
mean(var_vec) # just pulled out z which is rnorm(1 observation, mu_1 which is eq5 from our uninformative mu=0 and sigsq=10000, square root of sigma.sq_1) Know why square root? (R wants SD, not var)
mean(mean_vec)

```




<br>

#### IV. Writing a sampler

Now execute these steps:

1. Set up a matrix for storing samples from the posterior distribution of the mean. The number of rows should equal the number of chains (3) and number of columns should equal the number of iterations (10,000). Do the same thing for storing samples from the posterior distribution of the variance. 

```{r, echo = key, include = key}
n.iter = 10000
n.chains = 3 #replicates?
pvar = matrix(nrow = n.chains, ncol = n.iter)
pmean = matrix(nrow = n.chains, ncol = n.iter)
```

2. Assign initial values to the first column of each matrix, a different value for each of the chains. These can be virtually any value within the support of the random variable. 
   
To look for the fuzzy caterpillar! Need to start chains above, close to, and below the assumed mean. Hoping for convergence, that the variance among chains is less than the variance within one chain.          
```{r, echo = key, include = key}
pmean[1:3, 1] =c (50, 20, 1) #selected 50, 20, 1 for first column of the three chains
pvar[1:3, 1] = c(10, 5, .1) #pvar starting at 10, 5, and 0.1
```

3. Set up nested `for` loops to iterate from one to the total number of iterations for each of the three chains for each parameter. Use the conjugate functions `draw_mean` and `draw_var` to draw a sample from the distribution of the mean using the value of the variance at the current iteration. Then make a draw from the variance using the current value of the mean. Repeat. Assume vague priors for the mean and variance:

$$[\,\theta\,] = \textrm{normal}(\,\theta \mid (0, 10000\,)$$
$$[\,\varsigma^{2}\,] = \textrm{inverse gamma}(\,\varsigma^{2} \mid .001, .001\,)$$

```{r, echo = key, include = key}
for(t in 2:n.iter){
  for (j in 1:n.chains){
		pmean[j, t] = draw_mean(mu_0 = 0, sigma.sq_0 = 10000, varsigma.sq = pvar[j, t - 1], y = y)$z
		pvar[j, t] = draw_var(alpha_0 =.001, beta_0 = .001, theta = pmean[j ,t], y = y)$z
    #Uncomment to reverse order of draws required by problem 8. Note the change in indexing for pvar on the rhs.
    pvar[j, t] = draw_var(alpha_0 =.001, beta_0 = .001, theta = pmean[j ,t-1], y = y)$z
     
    pmean[j, t] = draw_mean(mu_0 = 0, sigma.sq_0 = 10000, varsigma.sq = pvar[j, t - 1], y = y)$z
  }		
}
```



#### V. Trace plots and plots of marginal posteriors

1. Discard the first 1000 iterations as burn-in. On the same figure, plot the value of the mean as a function of iteration number for each chain. This is called a trace plot. 

```{r, fig.widght = 6, fig.height = 5, fig.align = 'center', echo = key, include = key}
burnin = 1000
samplesKept <- (burnin+1):n.iter

plot(samplesKept, pmean[1, samplesKept], typ = "l", ylab = expression(theta), xlab = "Iteration", col = "yellow")
lines(samplesKept, pmean[2, samplesKept], typ = "l", col = "red")
lines(samplesKept, pmean[3, samplesKept], typ = "l", col = "green")
```

2. For all chains combined, make a histogram of the samples of the mean retained after burn-in. Put a vertical line on the plot showing the generating value.   

```{r, fig.widght = 6, fig.height = 5, fig.align = 'center', echo = key, include = key}
hist(pmean[, samplesKept], breaks = 100, freq = FALSE, main = expression(theta), xlim = c(95, 105), xlab = "Value of MCMC samples", col = "gray")
lines(density(pmean[, samplesKept]), col = "red", lwd = 3)
abline(v = theta, lty = "dashed", col = "blue", lwd = 4)
```

3. Repeat steps 1-2 for the variance.   

```{r, fig.widght = 6, fig.height = 5, fig.align = 'center', echo = key, include = key}
plot(samplesKept, pvar[1, samplesKept], typ = "l", ylab = expression(varsigma^2), xlab = "Iteration", col = "yellow")
lines(samplesKept, pvar[2, samplesKept], typ = "l", col = "red")
lines(samplesKept, pvar[3, samplesKept], typ = "l", col = "green")

hist(pvar[, samplesKept], breaks = 100, freq = FALSE, main = expression(varsigma^2), xlab = "Value of MCMC samples", col = "gray")
lines(density(pvar[, samplesKept]), col = "red", lwd = 3)
abline(v = varsigma.sq, lty = "dashed", col = "blue", lwd = 4)
```

4. For both $\theta$ and $\varsigma^{2}$, calculate the mean of all the chains combined and its standard deviation. Interpret these quantities.    

```{r, echo = key, include = key}
mean(pmean[, samplesKept])
sd(pmean[, samplesKept]) 
mean(pvar[, samplesKept])
sd(pvar[, samplesKept]) 
```
   

We learned in the lecture on basic probability that the first moment of a distribution can be approximated by computing the mean of many random draws from the distribution and the second central moment can be approximated by computing the variance of many random draws from the distribution.   

5. Compare the standard deviation of the posterior distribution of $\theta$ with an approximation using the standard deviation of the data divided by the square root of the sample size.  What is this approximation called in the frequentist world? 

```{r, echo = key, include = key}
sd(y)/sqrt(length(y))
```
   

The standard deviation of the posterior distribution of $\theta$ closely matches the approximation known as the standard error in frequentist lingo.   

6. Vary the number of values in the simulated data set, e.g., n = 10,100,1000. What happens to the mean and variance of the posterior distributions as $n$ gets large? We do not exactly recover the generating values of $\theta$ and $\varsigma^{2}$ when $n$ is small and the variance is further away from its generating value than the mean is. Why?    

The mean of the posterior distribution of $\theta$ and $\varsigma^2$ will not match the generating value when the sample size is small because simulated data represent one realization of stochastic process. This stochasticity can result in means and variances in small data sets that are quite different from the generating values. The variance is further away because it is more sensitive to extreme values that are rarely included in the simulated data.   

7. Make the burnin = 1 instead of 1000. Does this change your results? Why or why not?   
 

Gibbs updates are extremely efficient.  Convergence is virtually immediate for this simple, two parameter model.  This will not be true for accept-reject updates or for Gibbs updates with many parameters.

The MCMC algorithm is not sensitive to the order that samples are drawn from the full-conditionals.  

#### Write your own  


     1. Draw the DAG
     2. Write the posterior proportional to the joint distributions with full-conditional distributions for each unobserved quantity  
     3. Make a list of full-conditionals that can be computed analytically (conjugate priors) and full-conditionals that require Markov chain sampling method.  
     4. Write the interative function to sample from the conditional posteriors values for all the parameters and latent quantities to be estimated.   

  
#### For our bear example 

Agree on DAG and posterior $\propto$ likelihood and prior. Take easy appraoch, normal-normal conjugate distribution. 


We can alter the functions slightly to return the distribution instead of one sample. We can do this because this is a conjugate prior and can be computed analytically. This will speed up an MCMC. As a _Gibbs update_ don't use the ratio of the current and newly proposed, just update each value (because they're really smart choices).  
From page 168, formulas 7.3.40 and 7.3.41   
```{r}
# normal likelihood with normal prior conjugate for mean, assuming variance is known
# mu_0 is prior mean
# sigma.sq_0 is prior variance of mean
# varsigma.sq is known variance of data, based on average = 78.1 and variance = 10
#varsigma.sq in equation= sigma_p in our example
# Collect a sample of 15 bear territory sizes from our data
y <- rnorm(15, 78.1, sqrt(10)) #standard deviation = sqrt of sigma squared

# Calculate the probability density function of the likelihood
likelihood <- density(y)
h <- hist(y, plot=FALSE)

# Draw the probability density function of the prior
#vec <- seq(50,90,by=0.001)
#prior <- dnorm(vec, 70, sqrt(12)) #prior study
prior_0 <- rnorm(1000, 70, sqrt(12))

# This works because we assume we know varsigma "..which is vertually never the case" -Hobbs and Hooten 2015, pg 168
draw_distributionmean <- function(mu_0, sigma.sq_0, varsigma.sq, y){
  n <- length(y)
  mu_1 =((mu_0 / sigma.sq_0 + sum(y)/varsigma.sq)) / 
    (1/sigma.sq_0 + length(y)/varsigma.sq)
  sigma.sq_1 = 1/(1 / sigma.sq_0 + length(y) / varsigma.sq)
  z = rnorm(1000, mu_1, sqrt(sigma.sq_1))
  param = list(z = z, mu_1 = mu_1, sigma.sq_1 = sigma.sq_1)
  param
  }

posterior <- draw_distributionmean(70,12,10,y)
  
  
plot(density(prior_0), type="l", ylim=c(0,max(density(posterior$z)$y)),
     xlim=c(min(c(h$breaks,prior_0)),max(c(h$breaks,prior_0))), #min and max of prior and data distributions
     main="Distributions") 
hist(y, freq=FALSE, add=TRUE, col=rgb(0,0,1,0.15))
lines(density(y), col="blue")
lines(density(posterior$z),col="orange")
#text(posterior$mu_1+1,max(density(posterior$z)$y),"Posterior")
#text(mean(y)+1,max(h$density),"data")
#text(mean(prior_0)-1,max(density(prior_0)$y),"prior")
legend("topleft", lty=1, col=c("orange","blue","black"), 
       legend = c("Posterior","Likelihood","Prior"))
```


# <http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/> code to R
```{r}
#mu ~ normal
calc_posterior_analytical <-
  function(data,mu_0,sigma_0,varsigma){
    n <- length(data)
    mu_post <- (mu_0/sigma_0 + sum(data)/varsigma)/
      ((1/sigma_0) + (n/varsigma))
    sigma_post <- 1/((1/sigma_0) + (n/varsigma))
    out <- c(mu_post,sigma_post)
    out
  }

# y is our territory data from above; sigma_0 = 12 from other study; mu_0 is 70; varsigma is 10 (our mu was 78.1) 
bear.posterior <- calc_posterior_analytical(y, 70, 12, 10)

plot(density(rnorm(10000,bear.posterior[1],sqrt(bear.posterior[2]))),
     main="Analytical posterior",
     xlab="mu", ylab="belief")

```
Likelihood: true parameter is x, you define a likelihood curve based on that true parameter i.e. Poisson(y|x) or normal(y|mu,var)



Without a conjugate prior  
```{r}
# mu ~ normal
#Sampler 
sampler <- function(data, samples, proposal_width, prior_mu,prior_sig2,sigma_p){
  chain <- c()
  prior_data_pdf <- density(rnorm(10000,prior_mu,sqrt(prior_sig2)))
  mu_start <- rnorm(1, prior_mu, prior_sig2) #Pick random starting mu 
  chain <- mu_start
  mu_current <- mu_start

  for(i in 2:samples){
    likelihood_current <- prod(dnorm(data,mu_current,sqrt(sigma_p)))
    #suggest a dumb Metropolis new position
    mu_proposal <- rnorm(1,mu_current, sqrt(proposal_width)) 
    likelihood_proposal <- prod(dnorm(data,mu_proposal,sqrt(sigma_p)))

    #Prior
    prior_current <- dnorm(mu_current,prior_mu, sqrt(prior_sig2))
    prior_proposal <- dnorm(mu_proposal,prior_mu, sqrt(prior_sig2))
    p_current <- likelihood_current*prior_current
    p_proposal <- likelihood_proposal*prior_proposal
    
    # Accept proposal?
    R <- p_proposal/p_current
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) mu_proposal <- mu_current
      }
    chain[i] <- mu_proposal
    mu_current <- mu_proposal #which is either same if rejected or new if accepted
  }
  
  chain
}

```


```{r}
test <- sampler(data=y,samples=1000000,proposal_width = 10000,
                prior_mu=70,prior_sig2=12, sigma_p=10)

test[1]
plot(density(test), main="")
points(test[1],0.1)

plot(test[(length(test)-1000):length(test)],type="l") #the trace
plot(test,type="l")

```


```{r}

gamma_y <- rgamma(15, (70^2)/12,70/12)
plot(density(gamma_y))

test_gamma <- sampler(data=gamma_y,samples=100000,proposal_width = 9,
                prior_mu=70,prior_sig2=12, sigma_p=10)

test_gamma[1]
plot(density(test_gamma), main="")


```

In progress
```{r, eval=FALSE}

lognorm_y <- rlnorm(15, log(70),sqrt(log()))
plot(density(gamma_y))

test_lognorm <- sampler(data=gamma_y,samples=100000,proposal_width = 9,
                prior_mu=70,prior_sig2=12, sigma_p=10)

test_gamma[1]
plot(density(test_gamma), main="")


```


# Foraging Trees of Flammulated Owls  
```{r}
forage <- read.csv("P:/My Documents/UCDenver_phd/HBA_Spring2018/forage.csv")
library(rethinking)
library(fitdistrplus)
library(rstan)
```

Trees avaliable to birds, not used vs. trees used  
Bernoilli(p) probability that the tree was used, what's predictive of probability of use   
Stan is hamiltonian sampler. Physics model to let parameters move around sample space. Guess with momentum in a dirction and that changes. Calculates time in areas, momentum along gradient, less likely to get stuck even with messy posterior. 
Have to pass code in from another coder (like rstan)   
library(rethinking) translates code from more R like to stan text string.   
    fitdist helps pick a distribution the variables are coming from...  
    cbi_test <-   #burn survarity, try to fit nomral using moment matching  Can propose multiple and pick one that fits best   
    P-P is probabilities (instead of quantiles)
    Look at AIC and then look at the QQ plot. 
    Then scale by difference in normal mean vs. the estimate of the fit distribution   
    
Stan is less effiecent if you keep extraneous columns that are not using. 
Rethinking package should be able to give you the stan code (to learn and understand what it's doing)
       <http://xcelab.net/rm/statistical-rethinking/>

Something I want to come back and read: <http://bayesfactor.blogspot.com/> 

```{r}
forage$terr_in <- coerce_index(forage$terr)
#make dummy variables
forage$psme <- as.numeric(forage$foc_sp == 1)
forage$pipo <- as.numeric(forage$foc_sp == 2)
forage$potr <- as.numeric(forage$foc_sp == 3)
forage$slope_b <- as.numeric(forage$slope_pos == 1)
forage$slope_l <- as.numeric(forage$slope_pos == 2)
forage$slope_u <- as.numeric(forage$slope_pos == 3)
forage$slope_t <- as.numeric(forage$slope_pos == 4)

#transform cclos to proportion instead of percentage
forage$cclos <- (forage$cclos/100)
#fit predictor ditributions
fd.dens.over <- fitdist(forage$dens_over, distr = "lnorm", method = "mle")
summary(fd.dens.over)

plot(fd.dens.over)



```

```{r}
fd.dens.under <- fitdist(forage$dens_under, distr = "lnorm", method = "mle")
summary(fd.dens.under)
```

  
  
      1. Posterior predictive check - is the distribution of the posterior more extreme than distribution of the data?  
        a) test statistic (mean, var, 
$$\chisquare$$ )
      2.     
      

```{r, eval=FALSE}
install.packages("MCMCpack")
library(MCMCpack)
```


Without a conjugate prior, Bayesian p-Values  
```{r}
# mu ~ normal
#Sampler 

#log likelihood instead of many time numbers. 

sampler2 <- function(data, samples, proposal_width, prior_mu,prior_sig2,sigma_p){
  chain <- c()
  chain.sig <- c()
  T.k <- c()
  #prior_data_pdf <- density(rnorm(10000,prior_mu,sqrt(prior_sig2)))
  mu_start <- rnorm(1, prior_mu, prior_sig2) #Pick random starting mu 
#  sig_start <- actuar::rinvgamma(1, (prior_sig2/proposal_width)+2,
#                                 prior_sig2*((prior_sig2/proposal_width)+1))
  sig_start <- prior_sig2
  chain <- mu_start
  chain.sig <- sig_start
  #initial indicator value for test statistic (mean) for Bayesian p-value
  n <- length(data)
  ndraws <- rnorm(n,mu_start)
  # a little clunky, should allow for loop 1:samples....
  T.k[1] <- if(mean(ndraws)>=mean(data)){
      1
    } else {
      0
    }
  mu_current <- mu_start
  sig_current <- sig_start

  for(i in 2:samples){
    ###########
    # Keep sigma sq known, pick a mu
    likelihood_current <- prod(dnorm(data,mu_current,sqrt(sigma_p)))
    #suggest a dumb Metropolis new position
    mu_proposal <- rnorm(1,mu_current, sqrt(proposal_width)) 
    likelihood_proposal <- prod(dnorm(data,mu_proposal,sqrt(sigma_p)))

    #Prior mu
    prior_current <- dnorm(mu_current,prior_mu, sqrt(prior_sig2))
    prior_proposal <- dnorm(mu_proposal,prior_mu, sqrt(prior_sig2))
    p_current <- likelihood_current*prior_current
    p_proposal <- likelihood_proposal*prior_proposal
    ###########
    
    # Accept proposal?
    R <- p_proposal/p_current
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) mu_proposal <- mu_current
      }
    chain[i] <- mu_proposal
    mu_current <- mu_proposal #which is either same if rejected or new if accepted
    
    ###########
    # Keep mu known, pick a sigma sq
    likelihood_current <- prod(dnorm(data,mu_current,sqrt(sig_current)))
    #suggest new sigma
    sig_proposal <- actuar::rinvgamma(1,(mu_current/sig_current)+2,
                                 mu_current*((mu_current/sig_current)+1)) 
    likelihood_proposal <- prod(dnorm(data,mu_current,sqrt(sig_proposal)))

    #Prior sigma
    prior_currentsig <- actuar::dinvgamma(sig_current,
                                          (prior_sig2/proposal_width)+2,
                                          prior_sig2*((prior_sig2/proposal_width)+1))
    prior_proposalsig <- actuar::dinvgamma(sig_proposal,
                                           (prior_sig2/proposal_width)+2,
                                           prior_sig2*((prior_sig2/proposal_width)+1))
    p_currentsig <- likelihood_current*prior_currentsig
    p_proposalsig <- likelihood_proposal*prior_proposalsig
    
    # Accept propsal?
    R <- p_proposalsig/p_currentsig
  #  if(is.na(R))
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) sig_proposal <- sig_current
      }
    chain.sig[i] <- sig_proposal
    sig_current <- sig_proposal 
    
    
      #Bayesian p-Value
    ndraws <- rnorm(n,mu_current)
    T.k[i] <- if(mean(ndraws)>=mean(data)){
      1
    } else {
      0
    }
    
    #Marginal Posterior Distributions
    

  }
  
  B_p <- sum(T.k)/samples
  out <- list(chain,chain.sig,T.k,B_p)
}

```


#Beta is not correct for sigma squared! It's not between zero and 1, oy!
```{r, eval=FALSE}
plot(dbeta(seq(0,1,by=0.01),
           ((prior_mu^2)-(prior_mu^3)-(prior_mu*sqrt(prior_sig2)))/sqrt(prior_sig2), 
           (prior_mu- (2*prior_mu^2)+(prior_mu^3)-sqrt(prior_sig2)+(prior_mu*sqrt(prior_sig2)))/
              sqrt(prior_sig2)))


# Alpha should be 0-1 and Beta should be 0-.5^2
estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

plot(dbeta(seq(0,1,by=0.01),
     estBetaParams(sqrt(prior_sig2),sqrt(prior_sig2))[[1]],
     estBetaParams(sqrt(prior_sig2),sqrt(prior_sig2))[[2]]))


estBetaParams(prior_mu,prior_sig2)[[1]]/estBetaParams(prior_mu,prior_sig2)[[2]]

xseq <- seq(prior_sig2-1,prior_sig2+200,by=0.01)
plot(xseq, 
     dinvgamma(xseq, (prior_mu/prior_sig2)+2,
               prior_mu*((prior_mu/prior_sig2)+1)),
     xlab = "sigma squared",
     ylab="density")

xseq <- seq(prior_sig2-10,prior_sig2+10,by=0.01)
plot(xseq, 
     dinvgamma(xseq, (prior_sig2/sqrt(proposal_width))+2,
               prior_sig2*((prior_sig2/sqrt(proposal_width))+1)),
     xlab = "sigma squared",
     ylab="density")
```


```{r}
test <- sampler2(data=y,samples=100000,proposal_width = 10,
                prior_mu=70,prior_sig2=12, sigma_p=10)

test[[1]][1]
plot(density(test[[1]]), main="")
points(test[[1]][1],0.1)

#Bayes_p
plot(density(test[[3]]), main="Pb > 0.1 and < 0.9 adequately represents the data")
points(test[[4]], 0.1)

plot(test[[1]][(length(test[[1]])-1000):length(test[[1]])],type="l") #the trace
plot(test[[1]],type="l")

plot(density(test[[2]]), main="")

```


#Full conditional Gibbs
[$\alpha$,$\beta$,*z*,$\varsigma$|*Y*] $\propto$ $\prod^n$~i=1~ $\prod^J$~j=1~ [y~i,j~ | z,$\sigma^2$][z|g($\alpha$,$\beta$,x),$\varsigma$][$\alpha$][$\beta$][$\varsigma$][$\sigma^2$]
```{r}
J<-15 #site
n<-100  #replicates
mu_prior <- 30
sigma_prior <- 0.2
y<-rnorm(J*n,mu_prior,sigma_prior)
x<-runif(J,1,150) #rainfall or some covariate


sampler_full <- function(data, samples, x, proposal_width, Metropolis_Hastings = TRUE, 
                         init_alpha,init_beta,init_sigma,init_varsigma){
  chain.alpha <- c()
  chain.beta <- c()
  chain.z <- c()
  chain.sig <- c()
  chain.varsigma <- c()
  n<-length(data)
  T.k <- c() #indicator test variable for Bayesian p-value
  T.k[1]<-0.5 #don't have a prior ... this isnt' quite right

    #Initial values
  chain.alpha[1]<-runif(1,0,init_alpha)
  chain.beta[1]<-runif(1,0,init_beta)
  chain.z[1]<-rlnorm(1,log(mean(data))-((1/2)*(log((init_sigma+(mean(data)^2))/mean(data)^2))),
                                              sqrt(log((init_sigma+mean(data)^2)/mean(data)^2)))
  chain.sig[1]<-rinvgamma(1,0.001,0.001)
  chain.varsigma[1]<-rinvgamma(1,0.001,0.001)
  
  for(i in 2:samples)
  # z: the true value
  #likelihood of y conditional on z and sigma square
  z_now <- chain.z[i-1]
  likelihood.now <- prod(dlnorm(data,log(z_now)-((1/2)*(log((chain.sig[i-1]+
                     (z_now^2))/z_now^2))),
                     sqrt(log((chain.sig[i-1]+z_now^2)/z_now^2))))
  z_proposal <- rlnorm(1,log(z_now)-((1/2)*(log((chain.sig[i-1]+(z_now^2))/z_now^2))),
                                              sqrt(log((chain.sig[i-1]+z_now^2)/z_now^2)))
  likelihood.proposal <- prod(dlnorm(data,log(z_proposal)-((1/2)*(log((chain.sig[i-1]+
                     (z_proposal^2))/z_proposal^2))),
                     sqrt(log((chain.sig[i-1]+z_proposal^2)/z_proposal^2))))
  prior_now <- dlnorm(z_now,log(z_now)-((1/2)*(log((chain.sig[i-1]+
                     (z_now^2))/z_now^2))),
                     sqrt(log((chain.sig[i-1]+z_now^2)/z_now^2)))
  prior_proposal <- dlnorm(z_proposal,log(z_proposal)-((1/2)*(log((chain.sig[i-1]+
                     (z_proposal^2))/z_proposal^2))),
                     sqrt(log((chain.sig[i-1]+z_proposal^2)/z_proposal^2)))
  p_now <- likelihood.now*prior_now
  p_proposal <- likelihood.proposal*prior_proposal
  
  if(Metropolis_Hastings == FALSE){
  # Hastings
  # Accept proposal?
    R <- p_proposal/p_now
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) chain.z[i] <- z_now
    } else {
      chain[i] <- z_proposal
    }
  } else {
    #Metropolis-Hastings updates
    R <- (p_proposal/p_now) * (dlnorm(z_now,log(z_proposal)-((1/2)*(log((chain.sig[i-1]+
                     (z_proposal^2))/z_proposal^2))),
                     sqrt(log((chain.sig[i-1]+z_proposal^2)/z_proposal^2)))/
                        dlnorm(z_proposal,log(z_now)-((1/2)*(log((chain.sig[i-1]+
                     (z_now^2))/z_now^2))),
                     sqrt(log((chain.sig[i-1]+z_now^2)/z_now^2))))
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) chain.z[i] <- z_now
    } else {
      chain[i] <- z_proposalz
    }
    
    }
  #Bayesian p-value, just for z or what? for each? want for each, no?
  ndraws<-rlnorm(n,log(z_now)-((1/2)*(log((chain.sig[i-1]+
                     (z_now^2))/z_now^2))),
                     sqrt(log((chain.sig[i-1]+z_now^2)/z_now^2)))
  T.k[i]<-if(mean(ndraws)>=mean(data)){
    1
  } else {
    0
  }
  
  alpha_now <- chain.alpha[i-1]
  #page 163, assume deterministic model is a Michaelis-Menten alpha*x/(beta+x)
  #if wanted a linear model something like alpha*x + beta? 
  alphaL <- (alpha_now*x)/(chain.beta[i-1]+x)
  likelihood.now <- dlnorm(log(chain[i]),log(alphaL)-((1/2)*(log((chain.sig[i-1]+
                     (alphaL^2))/alphaL^2))),
                     sqrt(log((chain.sig[i-1]+alphaL^2)/alphaL^2)))
  
  sig_start <- actuar::rinvgamma(1, (prior_sig2/sqrt(proposal_width))+2,
                                 prior_sig2*((prior_sig2/proposal_width)+1))

  #initial indicator value for test statistic (mean) for Bayesian p-value
  n <- length(data)
  ndraws <- rnorm(n,mu_start)
  # a little clunky, should allow for loop 1:samples....
  T.k[1] <- if(mean(ndraws)>=mean(data)){
      1
    } else {
      0
    }
  mu_current <- mu_start
  sig_current <- sig_start

  for(i in 2:samples){
    ###########
    # Keep sigma sq known, pick a mu
    likelihood_current <- prod(dnorm(data,mu_current,sqrt(sigma_p)))
    #suggest a dumb Metropolis new position
    mu_proposal <- rnorm(1,mu_current, sqrt(proposal_width)) 
    likelihood_proposal <- prod(dnorm(data,mu_proposal,sqrt(sigma_p)))

    #Prior mu
    prior_current <- dnorm(mu_current,prior_mu, sqrt(prior_sig2))
    prior_proposal <- dnorm(mu_proposal,prior_mu, sqrt(prior_sig2))
    p_current <- likelihood_current*prior_current
    p_proposal <- likelihood_proposal*prior_proposal
    ###########
    
    
    
    ###########
    # Keep mu known, pick a sigma sq
    likelihood_current <- prod(dnorm(data,mu_current,sqrt(sigma_p)))
    #suggest new sigma
    sig_proposal <- actuar::rinvgamma(1,(mu_current/sig_start)+2,
                                 mu_current*((mu_current/sig_start)+1)) 
    likelihood_proposal <- prod(dnorm(data,mu_current,sqrt(sig_proposal)))

    #Prior sigma
    prior_currentsig <- actuar::dinvgamma(sig_current,
                                          (prior_sig2/sqrt(proposal_width))+2,
                                          prior_sig2*((prior_sig2/sqrt(proposal_width))+1))
    prior_proposalsig <- actuar::dinvgamma(sig_proposal,
                                           (prior_sig2/sqrt(proposal_width))+2,
                                           prior_sig2*((prior_sig2/sqrt(proposal_width))+1))
    p_currentsig <- likelihood_current*prior_currentsig
    p_proposalsig <- likelihood_proposal*prior_proposalsig
    
    # Accept propsal?
    R <- p_proposalsig/p_currentsig
    accept <- if(R < 1){
      u <- runif(1, min = 0,max = 1)
      if(R < u) sig_proposal <- sig_current
      }
    chain.sig[i] <- sig_proposal
    sig_current <- sig_proposal 
    
    
      #Bayesian p-Value
    ndraws <- rnorm(n,mu_current)
    T.k[i] <- if(mean(ndraws)>=mean(data)){
      1
    } else {
      0
    }
    
    #Marginal Posterior Distributions
    

  }
  
  B_p <- sum(T.k)/samples
  out <- list(chain,chain.sig,T.k,B_p)
}

```



```{r}
install.packages("rjags")
library(rjags)
```

Exercise 3: using for loops  
write a code framgment to set vague normal priors for 5 regression coefficients dnorm(0,10e^-6)
```{r}

for(i in 1:n){
  b[i] ~ dnorm(0,10e-6)
}

```


#Code Scott's data and code for Jags from stan  
Could recode the priors 