---
title: "JAGS example"
author: "Michelle DePrenger-Levin"
date: "April 2, 2018"
output: html_document
---

```{r}
#install.packages("R2jags")
library(R2jags)
library(rjags)
library(coda)
```

```{r}
set.seed(12)
samplesize <- 50 
xc <- sort(rnorm(samplesize)) # covariate
#to make factor levels
xf <- sample(c(0,1), size = samplesize, replace = T) # factor

#design matrix
model.matrix(~xc*xf)

#y=mx+b
# set 'true' differences between groups 0 and 1
int_true_0 <- 30 # intercept for 0
int_true_m_diff <- 8 # difference between intercepts
slope_true_0 <- 10 # slope for 0
slope_true_m_diff <- -6 # difference between slopes

# predict with perfect model, mu intercept for true 0 level, add xf ... model matrix basically
mu <- int_true_0 + xf * int_true_m_diff + 
  (slope_true_0 + xf * slope_true_m_diff) * xc # true means

plot(xc,mu)

# add some noise around
sigma <- 5 # true standard deviation of normal distributions
y <- rnorm(samplesize, mean = mu, sd = sigma) # response

plot(xc,y)

# Combine into a data frame:
dat <- data.frame(x1 = xc, x2 = xf, y = y)
head(dat)

# these define the length and parameters and cofactors that are in the models,
# parameters are the alpha, beta, tau
jagsdata1 <- with(dat, list(y = y, x1 = x1, N = length(y)))
jagsdata2 <- with(dat, list(y = y, x1 = x1, x2 = x2, N = length(y)))

# as a function instead of out to a file and then back in. 
lm1_jags <- function(){
  # Likelihood:
  for (i in 1:N){
    y[i] ~ dnorm(mu[i], tau)
    mu[i] <- alpha + beta * x1[i]
  }
  # Priors:
  alpha ~ dnorm(0, 0.01)
  beta ~ dnorm(0, 0.01)
  sigma ~ dunif(0, 100)
  tau <- 1 / (sigma * sigma)
}

lm2_jags <- function(){
  # Likelihood:
  for (i in 1:N){
    y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
    mu[i] <- alpha[1] + x2[i] * alpha[2] + (beta[1] + beta[2] * x2[i]) * x1[i]
  }
  # Priors:
  for (i in 1:2){
    alpha[i] ~ dnorm(0, 0.01)
    beta[i] ~ dnorm(0, 0.01)
  }
  sigma ~ dunif(0, 100)
  tau <- 1 / (sigma * sigma)
}

init_values1 <- function(){
  list(alpha = rnorm(1), beta = rnorm(1), sigma = runif(1))
}

init_values2 <- function(){
  list(alpha = rnorm(2), beta = rnorm(2), sigma = runif(1))
}

params <- c("alpha", "beta", "sigma","deviance")

fit_lm1 <- jags(data = jagsdata1, inits = init_values1, parameters.to.save = params,
                model.file = lm1_jags, n.chains = 1, n.iter = 12000, n.burnin = 2000,
                n.thin = 10, DIC = F)
fit_lm2 <- jags(data = jagsdata2, inits = init_values2, parameters.to.save = params,
                model.file = lm2_jags, n.chains = 1, n.iter = 12000, n.burnin = 2000,
                n.thin = 10, DIC = F)
fit_lm1
fit_lm2

fit_lm2 <- jags(data = jagsdata2, inits = init_values2, parameters.to.save = params,
                model.file = lm2_jags, n.chains = 1, n.iter = 12000, n.burnin = 2000,
                n.thin = 10, DIC = F)
fit_lm2
#makes JAGS object as an MCMC object
lm1_mcmc <- as.mcmc(fit_lm1)
lm2_mcmc <- as.mcmc(fit_lm2)

fit_lm1 <- jags(data = jagsdata1, inits = init_values1, parameters.to.save = params,
                model.file = lm1_jags, n.chains = 3, n.iter = 12000, n.burnin = 2000,
                n.thin = 10, DIC = F)
fit_lm2 <- jags(data = jagsdata2, inits = init_values2, parameters.to.save = params,
                model.file = lm2_jags, n.chains = 3, n.iter = 12000, n.burnin = 2000,
                n.thin = 10, DIC = F)
fit_lm1
fit_lm2

lm1_mcmc <- as.mcmc(fit_lm1)
lm2_mcmc <- as.mcmc(fit_lm2)

gelman.diag(lm1_mcmc)
heidel.diag(lm1_mcmc)
raftery.diag(lm1_mcmc)
```


```{r}
#fit_lm1
#50 sample size 
#want to iterate over all the chain steps. Not the average
all.parameters <- fit_lm1$BUGSoutput$sims.matrix

mu.all <- c()
y.new <- list()
for(j in 1:3000){
  for(i in 1:50){
    
  #pick random point given the linear model
  mu.all[i] <- rnorm(1,all.parameters[j,1] + all.parameters[j,2]*xc[i],
                     all.parameters[j,4])
  #expected value of 
  #By row, how different is that 
  }
  #need average over chain 
  y.new[[j]] <- mu.all
}

#3000 rows, 50 columns
sim.data <- do.call(rbind,y.new)

CV <- apply(sim.data,1,function(x){
  cv <- sd(x)/mean(x)
  cv
})

#posterior predictice loss  

#average of indiviudal obervation on each parameter set
new.mus <- apply(sim.data, 2, mean)
new.vars <- apply(sim.data, 2, var)
Dsel <- sum(dat$y-new.mus)^2 + sum(new.vars)
Dsel

hist(CV, xlim=c(sd(y)/mean(y)-0.1,max(CV)), main="Coefficient of variation")
points(sd(y)/mean(y),100)
segments(sd(y)/mean(y),0,sd(y)/mean(y),100)
text(sd(y)/mean(y),150, labels="CV of data", cex=0.75)

#Bayesian P-value
T.k <- sapply(CV,function(x){
  if(x >= (sd(y)/mean(y))){
    1
  } else {
    0
  }
})

hist(T.k)
points(mean(T.k),100,col="red")
mean(T.k) #0.497 a good fit. 


#fit_lm2
#alpha_1, alpha_2, beta_1, beta_2, sigma, deviance

#50 sample size 
#want to iterate over all the chain steps. Not the average
all.parameters2 <- fit_lm2$BUGSoutput$sims.matrix

mu.all2 <- c()
y.new2 <- list()
for(j in 1:3000){
  for(i in 1:50){ 
                           # alpha[1]                   x2 * alpha[2]   +
  mu.all2[i] <- rnorm(1, all.parameters2[j,1] + xf[i]*all.parameters2[j,2] +
    # (beta[1]           +        beta[2] * x2[i]) * x1[i]
    (all.parameters2[j,3] + all.parameters2[j,4]*xf[i])*xc[i],
    all.parameters2[j,6])
  }
  y.new2[[j]] <- mu.all2
}

sim.data2 <- do.call(rbind,y.new2)
str(sim.data2)

new.mus2 <- apply(sim.data2, 2, mean)
new.vars2 <- apply(sim.data2, 2, var)
Dsel <- sum(dat$y-new.mus2)^2 + sum(new.vars2)
Dsel



CV2 <- apply(sim.data2,1,function(x){
  cv2 <- sd(x)/mean(x)
  cv2
})

hist(CV2, xlim=c(sd(y)/mean(y)-0.1,max(CV2)), col=rgb(.5,.5,0,0.15),
     main="Coefficient of variation")
hist(CV, add=TRUE, col=rgb(0,0,1,0.15))
points(sd(y)/mean(y),100)
segments(sd(y)/mean(y),0,sd(y)/mean(y),90)
text(sd(y)/mean(y),150, labels="CV of data", cex=0.75)
legend("topright", fill=c(rgb(0,0,1,0.15),rgb(.5,.5,0,0.15)), legend = c("LM1","LM2"))


T.k2 <- sapply(CV2,function(x){
  if(x >= (sd(y)/mean(y))){
    1
  } else {
    0
  }
})


mean(T.k2) #0.54
```


```{r}
alpha.sim <- fit_lm1$BUGSoutput$sims.list$alpha
beta.sim <- fit_lm1$BUGSoutput$sims.list$beta
sigma.sim <- fit_lm1$BUGSoutput$sims.list$beta

#assume entering a single set of alpha, beta, and sigma to the function
ynew <- function(alpha,beta,sigma,x){
  sapply(x,function(x){
    set.seed(1)
    rnorm(1, alpha+beta*x,sigma)
  })
}

#50 rows, 3000 columns
foo <- sapply(1:length(alpha.sim), function(i){
  out <- ynew(alpha.sim[i],beta.sim[i],sigma.sim[i],xc)
})

CV.foo <- apply(foo,2,function(x){
  cv2 <- sd(x)/mean(x)
  cv2
})

T.k.foo1 <- sapply(CV.foo,function(x){
  if(x >= (sd(y)/mean(y))){
    1
  } else {
    0
  }
})


hist(T.k.foo1)
points(sd(y)/mean(y),100,col="red")

alpha1 <- fit_lm2$BUGSoutput$sims.list$alpha[,1]
alpha2 <- fit_lm2$BUGSoutput$sims.list$alpha[,2]
beta1 <- fit_lm2$BUGSoutput$sims.list$beta[,1]
beta2 <- fit_lm2$BUGSoutput$sims.list$beta[,2]
sigma2 <- fit_lm2$BUGSoutput$sims.list$sigma[,1]
x1 <- dat$x1
x2 <- dat$x2

ynew2 <- function(alpha1,alpha2,beta1,beta2,sigma,x1,x2){
      set.seed(1)
  mapply(function(x1,x2){
    rnorm(1, alpha1+x2*alpha2 + (beta1+beta2*x2)*x1,sigma)
  }, x1, x2)
}

#50 rows, 3000 columns
foo2 <- sapply(1:length(alpha1), function(x){
  out <- ynew2(alpha1[x],alpha2[x],beta1[x],beta2[x],sigma2[x],x1,x2)
})

#Need to apply over columns
CV.foo2 <- apply(foo2,2,function(x){
  cv2 <- sd(x)/mean(x)
  cv2
})

T.k.foo2 <- apply(foo2,2,function(x){
  if((sd(x)/mean(x)) >= (sd(y)/mean(y))){
    1
  } else {
    0
  }
})

cv <- sd(dat$y)/mean(dat$y)

hist(CV.foo)
abline(v = cv)

hist(CV.foo2)
abline(v = cv)


hist(CV.foo2, col=rgb(.5,.5,0,0.25), xlim = c(min(CV.foo2),max(CV.foo)),
     main="Coefficient of variation")
hist(CV.foo, add=TRUE, col=rgb(0,0,1,0.25))
points(sd(y)/mean(y),100)
segments(sd(y)/mean(y),0,sd(y)/mean(y),100)
text(sd(y)/mean(y),100, labels="CV of data", cex=0.75)
legend("topleft", fill=c(rgb(0,0,1,0.25),rgb(.5,.5,0,0.25)), legend = c("LM1","LM2"))



```

#look for JAGS eg 2
Now do WAIC  

```{r}


```
