---
title: "Lab 8 - design matrix"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 6
  html_notebook: default
  html_document: default
---

This lab offers practice with design matrices and an introduction to summaries of the lm() function. Let's work with the spider data, described in 5.8

```{r}
spider <- read.csv("P:/My Documents/BDA_Spring2018/spider_wolff_gorb_2013.csv")
```

***

**Q1.** Create the design matrix for modeling the effect of different leg sets on the observed friction coefficient. Provide only the R code for doing this (it's a long matrix), and write a few sentences to describe the components of the matrix (i.e. say what comprise the rows, columns, and values in each cell).
```{r}
spider.dm <- model.matrix(spider$friction~factor(spider$leg)+0) #as a boolean 
spider.dm[c(67:70,99:100,240:241),] #subset of the design matrix
```
    This design matrix acts as a boolean matrix setting the value in a cell to 1 when that individual (row) has the friction value for the corresponding leg pair in the appropriate column. Column 1 ("L1") has a 1 in rows with a cooresponding "L1" in the row of the spider data frame leg column and a 0 in all other rows. Likewise, column 2 of the design matrix only has 1s in rows for which the spider data frame leg column is "L2". This pattern is continued for "L3" and "L4".  
    



**Q2.** How many parameters in this design? Explain what they are and what information they provide, referencing the design matrix you produced in Q1. Then provide the parameter matrix.
   
There are four parameters in this design. The intercept, $\beta$~0~, which is the mean of friction from L1 leg pair. $\beta$~1~ is the offset from the intercept to the mean of L2. legL3, $\beta$~2~, is the effect size of L3 from L1 ($\beta$~0~ + $\beta$~2~ = mean friction of L3). The final parameter is $\beta$~3~ which is the effect size of L4 (the offset from L1 to L4).   
$$
{\left(\begin{array}{cc}
  \beta_{0}\\
  \beta_{1}\\
  \beta_{2}\\
  \beta_{3}
  \end{array}\right)
}
$$

***

lm() handles all that linear algebra we did in lab last week and in class this week. It outputs a lot of information. Let's take a look at a model for the effect of pushing vs. pulling (movement type) on the friction coefficient.

```{r}
modelfit <- lm(friction~type, data=spider)
str(modelfit)
```

Lots of results. The first few are the most commonly used. Coefficients are the $\hat{\beta}$ of course. Residuals are the $\epsilon$. The data, of course, provide the X and Y. So why all that other stuff? Much of it is beyond the scope of this course; we'll just look at summaries using the summary.lm() function. Note: can also just use summary() and R will recognize that it's an lm object...

```{r}
summary(modelfit)
```

Nice and tidy. First, R reminds us which model we fit. Next it provides the 5-point summary for the distribution of the $\epsilon_i$. Next, it gives the $\hat{\beta_i}$, associated uncertainties, and the results for a t-test of the null hypothesis that the given coefficient value is equal to zero. It next provides the standard error of the distribution of $\epsilon_i$ (and the denominator value so that a person could compute the standard deviation instead). Next is the $R^2$ and adjusted $R^2$. We can peek at the summary.lm function to figure out how $R^2$ and adjusted $R^2$ are computed:

1) $R^2 = \frac{MSS}{(MSS + RSS)}$
2) $adj.R^2 = 1 - (1 - r.squared) * \frac{(n - df.int)}{rdf}$

MSS is the mean sum of squares. RSS is the residual sum of squares. df.int is 1 if an intercept term is estimated, 0 if not. rdf is the residual degrees of freedom (N-p).

I inlcude this only because students always ask about the adjusted $R^2$. More generally, $R^2$ is a slipperly statistic, and it's often times much less useful that it's touted as being. We'll discuss the computations and interpretations in class. Some will benefit from this, others not so much. And that's OK.

Finally, the F statistic is the test statistic for all linear models. It is used to test the null hypothesis that the data were generated by a random process centered on a single value for the mean, and variance $\sigma^2$. We'll cover that in more detail later.

***

**Q3.** Given the output for the linear model presented above - for the effect of movement type (push v. pull) on the friction coefficient - what is the mean value of the friction coefficient for the spider legs that were pushing?   
```{r}
mean(spider$friction[spider$type=="push"])
1.21121+(-0.77901)
```
   The legs that were pushing has a mean value of 0.43. This is the intercept plus the effect size of the type push estimate. 

**Q4.** Fit the linear model for the design you provided in Q1 and write down the value for the mean friction coefficient for each of the 4 leg sets.   
```{r}
lm(friction~leg,data=spider)

spider.lm <- lm(friction~leg,data=spider)
print("Mean friction coefficient for L1:")
 mean(spider$friction[spider$leg=="L1"]) #L1

print("Mean friction coefficient for L2:")  
 mean(spider$friction[spider$leg=="L2"]) #L2
 spider.lm$coefficients[1]+spider.lm$coefficients[2]

print("Mean friction coefficient for L3:")
 mean(spider$friction[spider$leg=="L3"]) #L3
 spider.lm$coefficients[1]+spider.lm$coefficients[3]

print("Mean friction coefficient for L4:") 
 mean(spider$friction[spider$leg=="L4"]) #L4
 spider.lm$coefficients[1]+spider.lm$coefficients[4]

```

**Q5.** Given what you understand of the data, does the way we've been fitting the model seem to make intuitive sense, or should we be forcing the intercept to be zero? Explain.  
  If forcing the intercept to be zero can give the mean instead of the effect size of each in comparison to the first parameter, then forcing the intercept makes sense. However, in some cases we are interested in the effect size or difference from a baseline value for treatments. 


***

Finally, let's consider a multi-variate model (two-way ANOVA) for the additive effects of leg set and movement type on the friction coefficient. We'll fit the simple main effects model without and interaction.


```{r}
twowayFit <- lm(friction ~ type + leg, data = spider)
summary(twowayFit)
```

***
```{r,echo=FALSE}
tail(model.matrix(spider$friction~spider$leg+spider$type+0),50)
```

**Q6.** Write the sample space for this experimental design. Hint: this will be the set of all possible "states" for leg set and movement type that can be observed in the experiment. It may help to look at the actual design matrix you developed in Q1.   
    
    The sample space contains the following 8 rows:  
```{r}
expand.grid(c("Pull","Push"),c("L1","L2","L3","L4"))
``` 


**Q7.** Use that sample space to interpret each of the $\hat{\beta_i}$. That is, write down the experimental condition for the intercept, for typepush, legL2, etc.   

   The experimental condition... 