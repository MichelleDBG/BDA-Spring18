---
title: "Lecture Notes"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 5
  html_notebook: default
  html_document: default
---



####*Math is Music; Statistics is Literature - or why are there no 6-year old novelists?*

I posted a paper with that title on Canvas. Read it if you're interested. It provides useful and approachable perspective on why it's so hard to learn (and teach) statistical thinking. It can't be done without context, and should never, ever, be subjected to rote recipes, such as the one described (mockingly) by Gigerenzer in his 2004 paper in the Journal of Socio-economics, titled "Mindless statistics":

#####*The null ritual:*

1. Set up a statistical null hypothesis of “no mean difference” or “zero correlation.” Don’t specify the predictions of your research hypothesis or of any alternative substantive hypotheses.
2. Use 5% as a convention for rejecting the null. If significant, accept your research hypothesis. Report the result as p < 0.05, p < 0.01, or p < 0.001 (whichever comes next to the obtained p-value).
3. Always perform this procedure.

*****

Today's discussion will be centered on the problems with how statistics is taught and applied in the biological and applied sciences. Most of the readings comprise a relatively recent encarnation of a long-sustained effort toward reform in these matters. If you are interested, even vaguely, in the philosophy of science, I'd encourage you to read all the posted papers, and to go look up the more recent works (papers, books, blogs, you tube videos, etc.) of the authors. It'll do wonders for fertilizing your mind, and will make you a more informed scientist, much better prepared to strike out on your own once you leave the confines of your advisor's lab...

Because I was hired to develop a set of quantitative courses for the department, I've paid a lot of attention to the recent grumblings about how an alarming number of professionals in medicine and biomedicine have little to no grasp of probability. It's taken me a while to connect the dots that span decades of sustained effort at reform designed to address this problem. One of the most startling things for me to have discovered was the rubbing out of NHST in the biomedical literature during the 1970s and 1980s... When I was a graduate student in the early 2000's, I thought it was novel for some of the ecology journals to write editorials discouraging the reporting of p-values. It's taken rather a longer time for reform to impact education. It wasn't until 2015 that the MCATs first added sections on statistical inference, and not until 2016 did the ASA issue the statement that we all read on the misuse of p-values. And now, here you are, in 2017, taking a class that's fully embraced the educational reform...

****

Thought it would be interesting to read some of the editorial statements made in the medical literature. The following excerpts are quoted in Fidler et al (posted on Canvas). I expect the class discussion to be rather more technical about specific problems and recommended solutions, but these passages provide some added context for those who want to do research in biology and the allied fields.

When Ken Rothman was assistant editor of the *American Journal of Public Health (AJPH)*, he wrote in his revise and submit letters to would-be AJPH authors:

**"All references to statistical hypothesis testing and statistical significance should be removed from the papers. I ask that you delete p values as well as comments about statistical significance. If you do not agree with my standards (concerning the inap- propriateness of significance tests) you should feel free to argue the point, or simply ignore what you may consider to be my misguided view, by publishing elsewhere."**

For Rothman, it was not an issue of intellectual freedom, it was just correcting mistakes as one would correct grammatical errors:

**"My revise and submit letters . . . were not a covert attempt to engineer a new policy, but simply my attempt to do my job as I understood it. Just as I corrected grammatical errors, I corrected what I saw as conceptual errors in describing data. (K.J. Rothman, personal communication, July, 2002)"**"

And later as founding editor of *epidemiology* he wrote:

**"When writing for Epidemiology, you can enhance your prospects if you omit tests of statistical significance... In Epidemiology, we do not publish them at all. Not only do we eschew publishing claims of the presence or absence of statistical significance, we discourage the use of this type of thinking in the data analysis, such as in the use of stepwise regression."**

... In 2000, Rothman as editor of Epidemiology did not publish a single p value, and 94% of articles reported CIs.

Soon after, the *British Medical Journal* rolled out a policy recommending CIs rather than p values, which consequently increased the percentage of articles reporting CIs from 4% to 62%:

**"The British Medical Journal now expects scientific papers submitted to it to contain confidence intervals when appropriate. It also wants a reduced emphasis on the presentation of P values from hypothesis testing. *The Lancet*, the *Medical Journal of Australia*, the *American Journal of Public Health*, and the *British Heart Journal*, have implemented the same policy, and it has been endorsed by the International Committee of Medical Journal Editors."**

By as early as 1988, over 300 medical and biomedical journals had notified the International Committee of Medical Journal Editors (ICMJE) of their willingness to comply with the guidelines for publication. On the matter of NHST, their guidelines instructed:

**"When possible, quantify findings and present them with appropriate indicators of measurement error or uncertainty (such as confidence intervals). Avoid sole reliance on statistical hypothesis testing, such as the use of p values, which fail to convey important quantitative information"**"

```{r}


```

