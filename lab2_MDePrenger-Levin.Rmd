---
title: "Lab 2 - classic parametric inference"
output:
  html_document: default
  html_notebook: default
  pdf_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 5
---

###*WTF is a null hypothesis test?*

####Decision Spaces
The standard frequentist interpretation of a p-value resulting from a t-test is deceptively complex. Your book does a good job of describing misconcpetions. The p-value is *not* the probability of the null hypothesis being true. It is *not* the probability of the alternative hypothesis being false. It is *not* the probability of random chance generating the data. It *is* the probability of random chance alone generating data as or more inconsistent with the conditions described by the assumed (stated) null hypothesis, *if and only if* that null hypothesis is true. It's a probability about data (*NOT* about hypotheses), but it only applies if the conditions of the null hypothesis are true (which is almost always known *a priori* to be false). The first excercise in our demo is to simulate just those conditions (i.e. we can force $H_0$ to be true) to see what the distribution of p-values would be *if* the null hypothesis *were in fact true*. Since we never can know whether it is true when we collect data, let's simulate the situation...

```{r}
# declare the values for some parameters that we'll use in our MC simulation
reps<-5000 # number of experiments to run
n<-20 # sample size for each treatment in our experiment
mu<-25 # population average for each treatment
sig<-3 # population standard deviation for each treatment
p<-NULL # declare a null vector to collect the p-vals
set.seed(100)
# now set up a loop for pedagogical clarity
for (i in 1:reps) {
  trt1<-rnorm(n,mu,sig) # take n samples from "trt1"
  trt2<-rnorm(n,mu,sig) # take n samples from "trt2", but use same distribution
  # i.e. parameters are identical for both sample distributions
  # that means the data were from the same exact random process
  # so mu1 = mu2 is true for our simulated data
  p[i]<-t.test(trt1,trt2)$p.value # do the t-test and harvest the p-values
}
hist(p) # plot the distribution of p-values
mean(p<0.05) # find the type I error for our experiments; every TRUE becomes 1, FASLE is 0, then the mean, so proportion of values with p-value less than 0.05;0.0546 of the time

```

#####Question 1
a. Does the shape of the histogram for p-values under the null hypothesis surprise you? Explain.
  - The p-values, the chance of producing data different than the stated mean and standard deviation used to create the data, should be very low and similar each time. This will produce a uniform distribution of p-values.   
b. Use the shape of the distribution to explain the observed type I error rate (0.05).
  - Type I error rate, rejecting the NULL when it is true (the two samples were taken from the same distribution), will be expected to happen a small proportion of the time (0.05).
c. Write a short paragraph explaining why the distribution of p is uniform when $H_0$ is true. To help with this, try copy-and-pasting the code chunk into your response. Edit the code so that the experimental design does *not* meet the assumption that $H_0$ is true, and evaluate the resulting histogram of p-values.  
```{r}
# declare the values for some parameters to not be the same for two different samples
reps<-5000 # number of experiments to run
n<-20 # sample size for each treatment in our experiment
mu<-25 # population average for each treatment, central tendency for normal distribution
sig<-3 # population standard deviation for each treatment
p<-NULL # declare a null vector to collect the p-vals
set.seed(100)
# now set up a loop for pedagogical clarity
for (moremu in 0:5){
  for (i in 1:reps) {
    trt1<-rnorm(n,mu,sig) # take n samples from "trt1"
    trt2<-rnorm(n,mu+moremu,sig) # take n samples from "trt2", but use same distribution
  # i.e. parameters are identical for both sample distributions
  # that means the data were from the same exact random process
  # so mu1 = mu2 is true for our simulated data
    p[i]<-t.test(trt1,trt2)$p.value # do the t-test and harvest the p-values
    }
  hist(p, breaks=100,main=bquote("Difference of " ~ alpha == .(moremu)~","
                                   ~ sigma == .(sig)))
}

```
    
  - When two samples are drawn from the same distribution there is still a chance that the average of a sample will differ from the mean of the distribution. This results in a type I error of rejecting the null when the null is true. As the difference in the mean of two normal distibutions becomes larger than the variance ($\sigma$^2^), samples taken from the two distributions will have a distribution of the p-values with a $\mu$ approaching zero and a decreasing $\sigma$. The magnitude of the difference in signal of $\mu$ compared to the noise in $\sigma$ will determine the chance of seeing a parameter greater than expected.      
  

****

Does the Type I error depend on sample size? We can set up a simulation to look for the effect of sample size on the type I error rate as compared to theoretical type I error rates. The following code runs a loop within a loop. Computationally, this is very inefficient, but here it's used to (hopefully) more clearly illustrate that we are simulating lots of tests for each of many different sample sizes. Each time through, we compute the number of type I mistakes that are made when we use the p<0.05 decision criterion.

Why are the type I rates lower for low sample sizes? We correct for this error with different distributions of t. Those low rates illustratethe Student t-test which fix seeing more values at or more extreme than the tails at small sample sizes. 
```{r}
p<-NULL
t1<-NULL
mu<-25
sig<-3
reps<-2000
samp<-50
set.seed(100)
for (n in 2:samp) { # iterate through a range of sample sizes
  for (r in 1:reps) { # do the experiment "reps" times for each sample size
    pop1<-rnorm(n,mu,sig) # take n samples from "pop1"
    pop2<-rnorm(n,mu,sig) # take n samples from "pop2", the exact same distribution
    p[r]<-t.test(pop1,pop2)$p.value # do the t-test and harvest the p-values
    }
  t1[n-1]<-mean(p<0.05) # harvest the type I error rate for experiments of size n
} # note the indexing trick above - why did we start with n=2? - To have an average of two, not a sample of 1, can't run a t-test on a sample of 1.  

# now plot the distribution of type 1 error
plot(2:samp,t1,xlab="sample size (n)",ylab="type I error rate")
# add line to indicate the expected rate (0.05)
abline(h=0.05,col="red")
```


#####Question 2
a. What does this plot suggest about the rule of thumb that n should be ~20-30 for reliable results? Be specific in your response.
  - The type I error rate is around the p-value cutoff of 0.05 over a fairly uniform distribution (domain) of sample sizes. This error rate does not improve at or beyond 20-30 sample size range. There is no evidence that 20-30 samples are any better than 10-20 samples. A sample size can be estimated to reach a desired statistical power by dermining how much of a difference one wants to detect. 
b. Add a new code chunk and adjust the code above to generate a comparable plot showing how type I errors change as a function of precision in the random process (i.e. walk across a range of values for sigma)  

```{r, add precision}
p<-NULL
t1<-NULL
mu<-25
sig<-3
reps<-2000
samp<-50
set.seed(100)
sigs <- seq(10,0.1,-.1)

for (n in 1:length(sigs)) { # iterate through a range of sigma
  for (r in 1:reps) { # do the experiment "reps" times for each sample size
    pop1<-rnorm(samp,mu,sigs[n]) # take 50 samples from "pop1"
    pop2<-rnorm(samp,mu,sigs[n]) # take 50 samples from "pop2", the exact same distribution
    p[r]<-t.test(pop1,pop2)$p.value # do the t-test and harvest the p-values
  }
  t1[n]<-mean(p<0.05) # harvest the type I error rate for experiments of size n
} # note the indexing trick above - why did we start with n=2? - To have an average of two, not a sample of 1, can't run a t-test on a sample of 1.  

# now plot the distribution of type 1 error
plot(sigs,t1,xlab="standard deviation (sigma)",ylab="type I error rate")
# add line to indicate the expected rate (0.05)
abline(h=0.05,col="red")
```
   
c. What does this new plot say about how type I error rates are impacted by precision? Explain.   
  - Type I error rates are not greatly impacted by precision. Even at high precision there is still a decent chance of seeing a sample mean value at or greater than a rare difference. There is some increase in the rate at very high variances. 

****

####T-test in action
Let's take a look at some simpler code for t-tests than what's presented in the text (section 2.14). To do this, we need to load the mice weight data frame using read.csv(). Make sure you have a locally stored copy of that file on your machine and in your working directory.

```{r}
mice<-read.csv("P:/My Documents/BDA_Spring2018/femaleMiceWeights.csv") # point this to your file
# see what variables are in the dataframe
names(mice)
# Diet is the treatment (explanatory variable) and Bodyweight is the response variable
t.test(mice$Bodyweight~mice$Diet)
# above code is shorthand for separating subsets of data by a factor variable
# in English: "variation in Bodyweight can be explained by variation in Diet"
# take the time to understand all of the output
# especially the effect size and associated confidence interval... 
26.83417-23.81333
```

#####Question 3
a. Report the difference in means (point estimate for effect size) and the 95% CI for that difference  
  - The difference in means is -3.02; 95% CI -6.085 - 0.043  
```{r}
library(ggplot2)
ggplot(mice, aes(Diet,Bodyweight))+
  geom_boxplot()+
  geom_point( position = position_jitter(w = 0.1, h = 0))+
  theme_bw()
  
```
  
b. Write a sentence to interpret the 95% CI on the difference  
  - The 95% CI is wider than the difference in means 
c. Report the value of the test statistic, the t-distribution parameter (df) and p-value   
  - The t-test resulted in t = -2.06, df = 20.24, and p-value = 0.053. 
d. Write a sentence to interpret the p-value  
  - The p-value (0.053) approaches significance at the 0.05 threshold.
e. Which gives more information (estimated difference and 95% CI or the value of t and p)? Explain.  
  - By the p-value and t statistic, it would appear that there is some evidence that the diet had a significant effect on the body weight. The estimated difference in means and 95% CI around that estimate would indicate that the difference in body weight cannot be explained by diet.The 95% CI of one contains the mean of the other. There is no evidence that that diet resulted in a signficant increase in weight. 

****

####T-test in theory

Perhaps the most important limitation of using p-values derives from using them for inference when a research team conducts just a single replicate of an experiment. Watch the following eye-opening video in advance of responding to the next question: https://www.youtube.com/watch?v=ez4DgdurRPg&t=10s


#####Question 4
a. Most undergraduate education in the sciences encourages students to explain research results by reasoning from "facts" covered in lecture or lab. Although very interesting and creative, such post-hoc story-telling excercises may be leading future researchers down a potentially futile path. One consequence is that instructors (or PIs) become conditioned to ask for an experiment/study to be re-done "correctly" when the result doesn't match the expectation. Based on what you saw in the video, how might you now respond to a professor or advisor who asks for a do-over because the results seemed incorrect?   
  - If the expectation is that the assertion that a study was 'incorrect' is because of an insignficant p-value then I would present my analyses with the confidence interval instead. I might examine the power provided by the sample size and report the power associated with the 95% CI to describe evidence supporting or refuting the expectation. Repeating the study, if possible, might be a great way to to undertand the distribution of the resulting paramenter.     
b. Describe ways that you might extend a lab excercise you've done (personally) so that the primary point of the video can be illustrated for students.    
  - If you had every student take their own data, calculate the t statistic and p-value and then pooled all the results and examined the distriution of p-values over the class. Then could discuss who felt they did the experiment correctly and why when everyone did the exact same thing. Then look at the distribution of 95% confidence intervals and determine which experiment was done correctly and why does the assesment of good/correct work change?   
